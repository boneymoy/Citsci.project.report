%*****************************************
\chapter{Verwandte Arbeiten}\label{ch:verwandte_arbeiten}
%*****************************************

Forschungsfrage beruht auf zwei ideen.
Verbesserung von HIDS auf System calls mit verwendung von NLP Verfahren
Verbesserung von HIDS auf sc durch verwendung von extra parametern
Zunächst auf Literatur eingehen, welche Grundlagen der anomaliebasierte HIDS auf System Calls 
Dann Erweiterungen durch mehr als nur die Sequenz von System Calls
Und dann auf verschiedene NLP Verfahren die in der Literatur bisher in HIDS mit sc eingesetzt wurden

\section{Anomaliedetektion mit System Calls}

\subsection{Sequenzen}

    \paragraph{Anfänge}
        (1996) Erste Veröffentlichung im Bereich der anomaliebasierten HIDS auf system calls wurde 1996 in~\cite{FORREST} präsentiert.
        Dabei wird TIDE verwendet lookahead pairs.

        (1997) Aber auch schon erster \ac{ML} Ansatz mit dem RIPPER Algorithmus welcher Regeln lernt von~\cite{LEE1997}
        Bei diesem Ansatz werden normale sowie anormale Sequenzen gelernt.

        (1998) Dann 1998 sequences of fixed length von~\cite{STIDE} welches eine Erweiterung von TIDE darstellt.

        (1999) von~\cite{STIDE_Alternatives} bereits Vergleich verschiedener Methoden auch unter Verwendung von Verfahren (ngram) aus der Textverarbeitung.


    \paragraph{Fortschritte}
        (2005) Anstatt Sequenzen untersuchen~\cite{FREQUENCY2} Frequenzen von System Calls.
        \cite{FREQUENCY2} \glqq bag of system calls \grqq\ von 2005.

        (2013) Darstellung der System Calls als Zustände von Kernelmodulen~\cite{SYSTEM_STATES} und dann Interaktionen der Zustände untersuchen.
        Der Vergleich von Wahrscheinlichkeiten des Auftretens von Zuständen wird dann zur Identifizierung von Anomalien genutzt.

        (2018) In~\cite{SYSCALL_GRAPHS} werden wie in~\cite{STIDE_Alternatives} ngramme verwendet, diese dienen aber als Knoten für sogenannte \textit{System Call Graphs}. 
        Die Übergangswahrscheinlichkeiten der einzelnen n-gramme stellen die Kanten dieses Graphs dar.
        Der Anomaliescore wird dann anhand der Übergangswahrscheinlichkeiten aus dem im Training aufgebauten Graphen abgelesen.

        (2015) Einsatz in Linux Containern~\cite{FREQUENCY1} oder (2018) in Cloud Lösungen~\cite{VM}

\subsection{Erweiterung der System Call Sequenzen}

    \begin{itemize}
        \item extra models for syscalls~\cite{ARGUMENTS} extended~\cite{MAGGI}
        \item further~\cite{ARGUMENTS2}
    \end{itemize}

\section{NLP in der Anomaliedetektion und HIDS}

\subsection{Vorverarbeitung}

\subsection{Algorithmik}

\paragraph{RNN}

\paragraph{LSTM}

2016 Kim~\cite{LSTMsys} 

2017 Dymshits~\cite{LSTMDYMSHITS2017} supervising multiple hosts
normalized bag of system calls, sehr großes embedding (ca. 300), Vektor von integers für festes Zeitfenster (z.B. 1sec)



\subsection{RNNs für HIDS mit System Calls}
    Überblick~\cite{NLPHIDSSWORNA2022}
    NLP Dinge auch Transformer
    \begin{itemize}
        \item contextual word embedding tecniques biderectional encoder representations from Transformers (BERT)~\cite{NLPBERT2018}
        \item semantics~\cite{SEMANTICSLAKSHMANAN2015}
        \item semantics~\cite{SEMANTICSCREECH2014}
        \item Word2Vec~\cite{W2VWUNDERLICH2019}
        \item impact of embedding~\cite{IMPACTOFEMBEDDINGWUNDERLICH2020}
    \end{itemize}
    RNN und LSTMs
    \begin{itemize}
        \item RNN~\cite{RNN/CNN}
        \item LSTM1~\cite{LSTMsys}
        \item LSTM2~\cite{LSTMSURATKAR2019} 
        \item LSTM3~\cite{NIU2020} 
        \item LSTM4~\cite{BIDIRECTIONALLSTMCHAWLA2019} 
        \item LSTM5~\cite{VARIATIONALLSTMBOUZAR2019} 
        \item LSTM6~\cite{RNNVEDBOUZAR2020} 
        \item LSTM trained with malicious and bening data~\cite{LSTMKIM2016} % 33
        \item LSTM as EDR with events~\cite{EVENTLSTMVASQUEZ2020}
        \item LSTM in Industrial Control System ICS~\cite{ICSLSTMFENG2017} % 34
        \item LSTM uusing system logs~\cite{LOGLSTMMIN2017} %35
        
        \item weiteres
    \end{itemize}
